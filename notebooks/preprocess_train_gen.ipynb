{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T03:40:17.464720Z",
     "iopub.status.busy": "2021-05-22T03:40:17.464198Z",
     "iopub.status.idle": "2021-05-22T03:40:18.093081Z",
     "shell.execute_reply": "2021-05-22T03:40:18.091605Z",
     "shell.execute_reply.started": "2021-05-22T03:40:17.464653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys, os\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "from utils.haiku_scrape import scrape_haiku, replace_all, detokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pickle import dump, load\n",
    "\n",
    "#TF,nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haiku bot\n",
    "\n",
    "### How to enforce stanza structure?\n",
    "- 3 different rnns, each feeding into the next?\n",
    "\n",
    "### Several approaches to enforce syllable rule:\n",
    "1. Let the RNN learn on its own and see if it figures it out automatically \n",
    "    - Though syllables as a latent variable has very little hints in written text, this might work better with audio data of haikus\n",
    "2. Restrict number of output units for each stanza as the max number of words/stanza from the data \n",
    "    - Then in output, iterate until syllable count satisfied `(while count_syl(stanza) != 5...`\n",
    "        - This might not be so computationally expensive depending on number of possibile syllables (check this)      \n",
    "3. Break data down into phoneme/syllable components and feed that through RNN\n",
    "\n",
    "### Data cleaning\n",
    "- Some post titles have a preface, amend this by taking all text after a colon?\n",
    "\n",
    "### Embeddings\n",
    "- transfer learning compare pre trained vs trained \n",
    "    - maybe find a poem embedding\n",
    "- vis with umap\n",
    "\n",
    "### Architectures \n",
    "- consider the bidirectional\n",
    "- consider train/val split with scikit\n",
    "\n",
    "# Resources\n",
    "- https://medium.com/analytics-vidhya/a-comprehensive-guide-to-build-your-own-language-model-in-python-5141b3917d6d\n",
    "- https://github.com/KhanradCoder/LearnKeras/blob/master/4.RNNs/TextGeneration.ipynb\n",
    "- [Jason word based 1](https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/)\n",
    "- [Jason word based 2](https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/)\n",
    "- [return seq](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T22:12:50.329878Z",
     "iopub.status.busy": "2021-05-21T22:12:50.329534Z",
     "iopub.status.idle": "2021-05-21T22:13:04.855862Z",
     "shell.execute_reply": "2021-05-21T22:13:04.854208Z",
     "shell.execute_reply.started": "2021-05-21T22:12:50.329840Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "828"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_set = scrape_haiku()\n",
    "h_set.append('Keep this in your mind. / Only after the clouds cry, / will the rainbow come') #good haiku but had trailing '/'\n",
    "len(h_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T22:13:04.859417Z",
     "iopub.status.busy": "2021-05-21T22:13:04.858639Z",
     "iopub.status.idle": "2021-05-21T22:13:05.053539Z",
     "shell.execute_reply": "2021-05-21T22:13:05.051970Z",
     "shell.execute_reply.started": "2021-05-21T22:13:04.859346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Money obsession / Unhealthy neglect of life / Better to live now',\n",
       " \"Hanging out with friends / It is always a good time / Don't forget the mask\",\n",
       " 'Each time the phone rings / I think Grandma and forget / You left in the spring',\n",
       " \"When I learned Morse code / I couldn't get restful sleep / The rain kept talking\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_set[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T22:13:10.560754Z",
     "iopub.status.busy": "2021-05-21T22:13:10.560257Z",
     "iopub.status.idle": "2021-05-21T22:13:11.021312Z",
     "shell.execute_reply": "2021-05-21T22:13:11.020204Z",
     "shell.execute_reply.started": "2021-05-21T22:13:10.560688Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_data(haikus,n=3):\n",
    "    \"\"\"n: n-gram size\"\"\"\n",
    "    train = [] #n_grams\n",
    "    for h in haikus:\n",
    "        tokens = word_tokenize(h)\n",
    "        for i in range(0,len(tokens)):\n",
    "            n_gram_list = tokens[i:i+n]\n",
    "            if len(n_gram_list) == n:\n",
    "                train.append(detokenize(n_gram_list))\n",
    "            else:\n",
    "                break\n",
    "    return train\n",
    "train = prepare_data(h_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-21T22:13:13.923037Z",
     "iopub.status.busy": "2021-05-21T22:13:13.922688Z",
     "iopub.status.idle": "2021-05-21T22:13:14.603101Z",
     "shell.execute_reply": "2021-05-21T22:13:14.601800Z",
     "shell.execute_reply.started": "2021-05-21T22:13:13.922997Z"
    }
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train)\n",
    "sequences = tokenizer.texts_to_sequences(train)\n",
    "sequences = [i for i in sequences if len(i) == 3] # removing entries with apostrophe for now\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "#separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instead of tf tokenizer could also do \n",
    "chars = sorted(list(set(processed_text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# or\n",
    "\n",
    "chars = sorted(list(set(data_new)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "def encode_seq(seq):\n",
    "    sequences = list()\n",
    "    for line in seq:\n",
    "        # integer encode line\n",
    "        encoded_seq = [mapping[char] for char in line]\n",
    "        # store\n",
    "        sequences.append(encoded_seq)\n",
    "    return sequences\n",
    "\n",
    "# encode the sequences\n",
    "sequences = encode_seq(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Sequential LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-19T23:19:46.036266Z",
     "iopub.status.busy": "2021-05-19T23:19:46.035455Z",
     "iopub.status.idle": "2021-05-19T23:19:48.441074Z",
     "shell.execute_reply": "2021-05-19T23:19:48.439939Z",
     "shell.execute_reply.started": "2021-05-19T23:19:46.036184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 2, 50)             130900    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 2, 100)            60400     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2618)              264418    \n",
      "=================================================================\n",
      "Total params: 546,218\n",
      "Trainable params: 546,218\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=50, input_length=seq_length),\n",
    "    LSTM(100,return_sequences=True),\n",
    "    LSTM(100),\n",
    "    Dense(100,activation = 'relu'),\n",
    "    Dense(vocab_size, activation = 'softmax')\n",
    "])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, batch_size=128, epochs=75)\n",
    "save = 1\n",
    "if save:\n",
    "    model.save('../models/model.h5')\n",
    "    dump(tokenizer, open('../models/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T03:41:48.229369Z",
     "iopub.status.busy": "2021-05-22T03:41:48.220600Z",
     "iopub.status.idle": "2021-05-22T03:41:49.354938Z",
     "shell.execute_reply": "2021-05-22T03:41:49.353821Z",
     "shell.execute_reply.started": "2021-05-22T03:41:48.229294Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('../models/model.h5',compile=False)\n",
    "tokenizer = load(open('../models/tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-22T03:46:07.952868Z",
     "iopub.status.busy": "2021-05-22T03:46:07.952295Z",
     "iopub.status.idle": "2021-05-22T03:46:09.074195Z",
     "shell.execute_reply": "2021-05-22T03:46:09.072776Z",
     "shell.execute_reply.started": "2021-05-22T03:46:07.952734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I called my\n",
      "\n",
      "legs upon the leaves ns died with harsh broken coming ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work\n"
     ]
    }
   ],
   "source": [
    "index_word = {v:k for k,v in tokenizer.word_index.items()}\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    output = []\n",
    "    text = seed_text\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([text])[0] # encode the text as integer\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre') #truncate sequences toa  fixed length (since reinputted)\n",
    "        yhat = model.predict_classes(encoded, verbose=False)\n",
    "        _word = index_word[yhat[0]] # map predicted word index to word\n",
    "        text += ' ' + _word\n",
    "        output.append(_word)\n",
    "    return ' '.join(output)\n",
    "        \n",
    "seq_length = len(train[0].split()) - 1 # also defined as X.shape[1] above\n",
    "seed_text = train[np.random.randint(0,len(train))]\n",
    "print(seed_text + '\\n')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 100)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Some weird oscillating output (even though I am going beyond haiku length wanted to see what it generates)\n",
    "\n",
    "```\n",
    "I called my\n",
    "\n",
    "legs upon the leaves ns died with harsh broken coming ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work ns i am stuck at work\n",
    "```\n",
    "\n",
    "- Need to figure out better tokens\n",
    "- Hyperparams\n",
    "    - see [this](https://stackoverflow.com/questions/56849552/lstm-getting-caught-up-in-loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
